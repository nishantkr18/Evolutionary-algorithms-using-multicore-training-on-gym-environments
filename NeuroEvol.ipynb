{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, torch, time, math, copy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleAI(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.fc = nn.Sequential(\n",
    "                        nn.Linear(4,128, bias=True),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Linear(128,2, bias=True),\n",
    "                        nn.Softmax(dim=1)\n",
    "                        )\n",
    "\n",
    "                \n",
    "        def forward(self, inputs):\n",
    "            x = self.fc(inputs)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    \n",
    "        # nn.Conv2d weights are of shape [16, 1, 3, 3] i.e. # number of filters, 1, stride, stride\n",
    "        # nn.Conv2d bias is of shape [16] i.e. # number of filters\n",
    "        \n",
    "        # nn.Linear weights are of shape [32, 24336] i.e. # number of input features, number of output features\n",
    "        # nn.Linear bias is of shape [32] i.e. # number of output features\n",
    "        \n",
    "        if ((type(m) == nn.Linear) | (type(m) == nn.Conv2d)):\n",
    "            torch.nn.init.xavier_uniform(m.weight)\n",
    "            m.bias.data.fill_(0.00)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_random_agents(num_agents):\n",
    "    \n",
    "    agents = []\n",
    "    for _ in range(num_agents):\n",
    "        \n",
    "        agent = CartPoleAI()\n",
    "        \n",
    "        for param in agent.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        init_weights(agent)\n",
    "        agents.append(agent)\n",
    "        \n",
    "        \n",
    "    return agents\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents(agents):\n",
    "    \n",
    "    reward_agents = []\n",
    "    env = gym.make(\"CartPole-v0\")\n",
    "    \n",
    "    for agent in agents:\n",
    "        agent.eval()\n",
    "    \n",
    "        observation = env.reset()\n",
    "        \n",
    "        r=0\n",
    "        s=0\n",
    "        \n",
    "        for _ in range(250):\n",
    "            \n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            r=r+reward\n",
    "            \n",
    "            s=s+1\n",
    "            observation = new_observation\n",
    "\n",
    "            if(done):\n",
    "                break\n",
    "\n",
    "        reward_agents.append(r)        \n",
    "        #reward_agents.append(s)\n",
    "        \n",
    "    \n",
    "    return reward_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_average_score(agent, runs):\n",
    "    score = 0.\n",
    "    for i in range(runs):\n",
    "        score += run_agents([agent])[0]\n",
    "    return score/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_agents_n_times(agents, runs):\n",
    "    avg_score = []\n",
    "    for agent in agents:\n",
    "        avg_score.append(return_average_score(agent,runs))\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate(agent):\n",
    "\n",
    "    child_agent = copy.deepcopy(agent)\n",
    "    \n",
    "    mutation_power = 0.02 #hyper-parameter, set from https://arxiv.org/pdf/1712.06567.pdf\n",
    "            \n",
    "    for param in child_agent.parameters():\n",
    "    \n",
    "        if(len(param.shape)==4): #weights of Conv2D\n",
    "\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    for i2 in range(param.shape[2]):\n",
    "                        for i3 in range(param.shape[3]):\n",
    "                            \n",
    "                            param[i0][i1][i2][i3]+= mutation_power * np.random.randn()\n",
    "                                \n",
    "                                    \n",
    "\n",
    "        elif(len(param.shape)==2): #weights of linear layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                for i1 in range(param.shape[1]):\n",
    "                    \n",
    "                    param[i0][i1]+= mutation_power * np.random.randn()\n",
    "                        \n",
    "\n",
    "        elif(len(param.shape)==1): #biases of linear layer or conv layer\n",
    "            for i0 in range(param.shape[0]):\n",
    "                \n",
    "                param[i0]+=mutation_power * np.random.randn()\n",
    "\n",
    "    return child_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_children(agents, sorted_parent_indexes, elite_index):\n",
    "    \n",
    "    children_agents = []\n",
    "    \n",
    "    #first take selected parents from sorted_parent_indexes and generate N-1 children\n",
    "    for i in range(len(agents)-1):\n",
    "        \n",
    "        selected_agent_index = sorted_parent_indexes[np.random.randint(len(sorted_parent_indexes))]\n",
    "        children_agents.append(mutate(agents[selected_agent_index]))\n",
    "\n",
    "    #now add one elite\n",
    "    elite_child = add_elite(agents, sorted_parent_indexes, elite_index)\n",
    "    children_agents.append(elite_child)\n",
    "    elite_index=len(children_agents)-1 #it is the last one\n",
    "    \n",
    "    return children_agents, elite_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_elite(agents, sorted_parent_indexes, elite_index=None, only_consider_top_n=10):\n",
    "    \n",
    "    candidate_elite_index = sorted_parent_indexes[:only_consider_top_n]\n",
    "    \n",
    "    if(elite_index is not None):\n",
    "        candidate_elite_index = np.append(candidate_elite_index,[elite_index])\n",
    "        \n",
    "    top_score = None\n",
    "    top_elite_index = None\n",
    "    \n",
    "    for i in candidate_elite_index:\n",
    "        score = return_average_score(agents[i],runs=5)\n",
    "        print(\"Score for elite i \", i, \" is \", score)\n",
    "        \n",
    "        if(top_score is None):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "        elif(score > top_score):\n",
    "            top_score = score\n",
    "            top_elite_index = i\n",
    "            \n",
    "    print(\"Elite selected with index \",top_elite_index, \" and score\", top_score)\n",
    "    \n",
    "    child_agent = copy.deepcopy(agents[top_elite_index])\n",
    "    return child_agent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.05 s, sys: 252 ms, total: 6.3 s\n",
      "Wall time: 6.32 s\n",
      "\n",
      "\n",
      "Generation  0  | Mean rewards:  21.665333333333333  | Mean of top 5:  48.86666666666666\n",
      "Top  20  scores [ 29 226 171 365  31 257  22 466 272 322 175 306 340 484 339 457 413 433\n",
      " 360 276]\n",
      "Rewards for top:  [56.666666666666664, 50.0, 47.666666666666664, 45.333333333333336, 44.666666666666664, 44.0, 43.333333333333336, 43.333333333333336, 43.0, 42.333333333333336, 41.666666666666664, 41.333333333333336, 41.0, 40.666666666666664, 39.666666666666664, 39.333333333333336, 39.0, 38.666666666666664, 38.333333333333336, 38.0]\n",
      "Score for elite i  29  is  37.4\n",
      "Score for elite i  226  is  26.2\n",
      "Score for elite i  171  is  17.8\n",
      "Score for elite i  365  is  18.4\n",
      "Score for elite i  31  is  27.0\n",
      "Score for elite i  257  is  35.6\n",
      "Score for elite i  22  is  24.0\n",
      "Score for elite i  466  is  19.8\n",
      "Score for elite i  272  is  22.0\n",
      "Score for elite i  322  is  24.0\n",
      "Elite selected with index  29  and score 37.4\n",
      "CPU times: user 6.24 s, sys: 228 ms, total: 6.47 s\n",
      "Wall time: 6.47 s\n",
      "\n",
      "\n",
      "Generation  1  | Mean rewards:  22.847333333333335  | Mean of top 5:  52.866666666666674\n",
      "Top  20  scores [344 469 310 106  55  85 207 350 358 108 407 459 167   5 196 348  81 427\n",
      "  98  45]\n",
      "Rewards for top:  [57.333333333333336, 56.333333333333336, 54.0, 48.333333333333336, 48.333333333333336, 47.0, 43.333333333333336, 43.0, 42.333333333333336, 42.333333333333336, 41.666666666666664, 41.0, 41.0, 40.333333333333336, 40.0, 40.0, 39.666666666666664, 39.666666666666664, 39.0, 38.666666666666664]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-b15ecd42a068>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# setup an empty list for containing children agents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mchildren_agents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melite_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_parent_indexes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melite_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# kill all agents, and replace them with their children\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-760252dbc6c4>\u001b[0m in \u001b[0;36mreturn_children\u001b[0;34m(agents, sorted_parent_indexes, elite_index)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mselected_agent_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted_parent_indexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted_parent_indexes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mchildren_agents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmutate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mselected_agent_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m#now add one elite\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-27c2141b7ff4>\u001b[0m in \u001b[0;36mmutate\u001b[0;34m(agent)\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mmutation_power\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "game_actions = 2 #2 actions possible: left or right\n",
    "\n",
    "#disable gradients as we will not use them\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "# initialize N number of agents\n",
    "num_agents = 500\n",
    "agents = return_random_agents(num_agents)\n",
    "\n",
    "# How many top agents to consider as parents\n",
    "top_limit = 20\n",
    "\n",
    "# run evolution until X generations\n",
    "generations = 1000\n",
    "\n",
    "elite_index = None\n",
    "\n",
    "for generation in range(generations):\n",
    "\n",
    "    # return rewards of agents\n",
    "    %time rewards = run_agents_n_times(agents, 3) #return average of 3 runs\n",
    "\n",
    "    # sort by rewards\n",
    "    sorted_parent_indexes = np.argsort(rewards)[::-1][:top_limit] #reverses and gives top values (argsort sorts by ascending by default) https://stackoverflow.com/questions/16486252/is-it-possible-to-use-argsort-in-descending-order\n",
    "    print(\"\")\n",
    "    print(\"\")\n",
    "    \n",
    "    top_rewards = []\n",
    "    for best_parent in sorted_parent_indexes:\n",
    "        top_rewards.append(rewards[best_parent])\n",
    "    \n",
    "    print(\"Generation \", generation, \" | Mean rewards: \", np.mean(rewards), \" | Mean of top 5: \",np.mean(top_rewards[:5]))\n",
    "    #print(rewards)\n",
    "    print(\"Top \",top_limit,\" scores\", sorted_parent_indexes)\n",
    "    print(\"Rewards for top: \",top_rewards)\n",
    "    \n",
    "    # setup an empty list for containing children agents\n",
    "    children_agents, elite_index = return_children(agents, sorted_parent_indexes, elite_index)\n",
    "\n",
    "    # kill all agents, and replace them with their children\n",
    "    agents = children_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def play_agent(agent):\n",
    "    try: #try and exception block because, render hangs if an erorr occurs, we must do env.close to continue working    \n",
    "        env_record = gym.make(\"CartPole-v0\")\n",
    "        \n",
    "#         env_record = Monitor(env, './video', force=True)\n",
    "        observation = env_record.reset()\n",
    "        last_observation = observation\n",
    "        r=0\n",
    "        for _ in range(3000):\n",
    "            env_record.render()\n",
    "            inp = torch.tensor(observation).type('torch.FloatTensor').view(1,-1)\n",
    "            output_probabilities = agent(inp).detach().numpy()[0]\n",
    "            action = np.random.choice(range(game_actions), 1, p=output_probabilities).item()\n",
    "            new_observation, reward, done, info = env_record.step(action)\n",
    "            r=r+reward\n",
    "            observation = new_observation\n",
    "            xpos = observation[0]\n",
    "            print(reward)\n",
    "            time.sleep(0.1)\n",
    "            if(xpos>=2.4 or xpos<=-2.4):\n",
    "                env_record.reset()\n",
    "\n",
    "        env_record.close()\n",
    "        print(\"Rewards: \",r)\n",
    "\n",
    "    except Exception as e:\n",
    "        env_record.close()\n",
    "        print(e.__doc__)\n",
    "        print(e.message)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_agent(agents[499])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
